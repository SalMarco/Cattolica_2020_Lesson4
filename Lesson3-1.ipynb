{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Introduction to Python\n",
    "================================\n",
    "\n",
    "Lesson 3 - Part1\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "\n",
    "In this lesson we will focus on Neural Network. The lesson is divided between an introduction an a technical part\n",
    "The topis that we'll cover are in the introduction are:\n",
    "\n",
    "  - Tensorflow\n",
    "  - Keras \n",
    "  \n",
    "For the technical part we will create some models for:\n",
    "\n",
    "  - Single class classification\n",
    "  - Multi class classification\n",
    "  - Custom model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Today there are many framework that allow to create a Neural Network, like:\n",
    "\n",
    "  - Tensorflow \n",
    "  - Theano\n",
    "  - Caffe\n",
    "  - MXNett\n",
    "  - CNTK\n",
    "  \n",
    "Among all of them, probabilly the most famous is Tensorflow.\n",
    "\n",
    "Considering the support, the community the resources available, Tensorflow is the engine chosen for this course.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensorflow\n",
    "\n",
    "[Tensorflow](https://www.tensorflow.org/) it was originally developed by Google, now it's open source.\n",
    "\n",
    "The definition from the web site is the following:\n",
    "\n",
    "*TensorFlow™ is an open source software library for high performance numerical computation. Its flexible architecture allows easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices. Originally developed by researchers and engineers from the Google Brain team within Google’s AI organization, it comes with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domains.*\n",
    "\n",
    "Google has deeply invested in this project, to the point that now in Google Cloud there are available some instances with GPU's (called TPU's) designed especially for TensorFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras\n",
    "\n",
    "In order to simply the definition of the Neural Network and to speed up the development time we'll not use TensorFlow directly.\n",
    "Instead we'll use [Keras](https://keras.io/).\n",
    "\n",
    "The definition from the website is the following:\n",
    "\n",
    "*Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "*\n",
    "\n",
    "As you can see from the description, Keras offers another advantage: with just a change of setting the same code can be used with:\n",
    "\n",
    "  - Tensorflow\n",
    "  - Theano\n",
    "  - CNTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 2.0\n",
    "\n",
    "Tensorflow 2.0 has been released on September 30th 2019.\n",
    "\n",
    "The main new feature in this new relese in that Keras' API are now the official High Level APIs of Tensorflow, directly supported by the project itself.\n",
    "\n",
    "Quoting the home page of Keras:\n",
    "\n",
    "*At this time, we recommend that Keras users who use multi-backend Keras with the TensorFlow backend switch to tf.keras in TensorFlow 2.0. tf.keras is better maintained and has better integration with TensorFlow features (eager execution, distribution support and other).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison between Keras and Tensorflow\n",
    "\n",
    "In order to undestand the advantage of using Keras, we will consider the code necessary to define the same network using TensorFlow directly or Keras.\n",
    "\n",
    "Before moving on we must remember what are the parameters involved with a neural network.\n",
    "\n",
    "Can you list me those parameters?\n",
    "\n",
    "  - num imp.\n",
    "  - num. layers\n",
    "  - num. out \n",
    "  - node in layers \n",
    "  - activation \n",
    "  - loss\n",
    "  - optimization\n",
    "  - epoch \n",
    "  - batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TensoFlow Multiclass\n",
    "\n",
    "Let's create a neural network to be used over the classic [mnsit](http://yann.lecun.com/exdb/mnist/) hand written digits with two hidden layer to classify samples in to the 10 possible classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet at 0x134d7add8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1030 17:11:46.189947 4574193088 deprecation.py:323] From <ipython-input-7-7f6d4416490f>:54: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 7764.5581, Training Accuracy= 0.383\n",
      "Step 2, Minibatch Loss= 10254.6035, Training Accuracy= 0.469\n",
      "Step 3, Minibatch Loss= 11284.6406, Training Accuracy= 0.508\n",
      "Step 4, Minibatch Loss= 13357.6621, Training Accuracy= 0.422\n",
      "Step 5, Minibatch Loss= 7776.8623, Training Accuracy= 0.578\n",
      "Step 6, Minibatch Loss= 5782.4629, Training Accuracy= 0.594\n",
      "Step 7, Minibatch Loss= 2082.1309, Training Accuracy= 0.719\n",
      "Step 8, Minibatch Loss= 2376.5703, Training Accuracy= 0.695\n",
      "Step 9, Minibatch Loss= 3301.7427, Training Accuracy= 0.703\n",
      "Step 10, Minibatch Loss= 1859.7058, Training Accuracy= 0.711\n",
      "Step 11, Minibatch Loss= 1238.5892, Training Accuracy= 0.758\n",
      "Step 12, Minibatch Loss= 1334.7411, Training Accuracy= 0.828\n",
      "Step 13, Minibatch Loss= 2748.2046, Training Accuracy= 0.688\n",
      "Step 14, Minibatch Loss= 2132.0815, Training Accuracy= 0.734\n",
      "Step 15, Minibatch Loss= 1643.2915, Training Accuracy= 0.820\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.7726\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 15\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                      Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras Multiclass\n",
    "\n",
    "Let's create a neural network to be used over the classic [mnsit](http://yann.lecun.com/exdb/mnist/) hand written digits with two hidden layer to classify samples in to the 10 possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.2291 - acc: 0.9319 - val_loss: 0.0957 - val_acc: 0.9699\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.0850 - acc: 0.9731 - val_loss: 0.0949 - val_acc: 0.9706\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 4s 70us/step - loss: 0.0533 - acc: 0.9835 - val_loss: 0.0843 - val_acc: 0.9745\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 3s 61us/step - loss: 0.0405 - acc: 0.9869 - val_loss: 0.0655 - val_acc: 0.9819\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 4s 67us/step - loss: 0.0348 - acc: 0.9885 - val_loss: 0.0673 - val_acc: 0.9814\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 4s 65us/step - loss: 0.0263 - acc: 0.9912 - val_loss: 0.0770 - val_acc: 0.9775\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 3s 64us/step - loss: 0.0216 - acc: 0.9929 - val_loss: 0.0719 - val_acc: 0.9806\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 3s 62us/step - loss: 0.0213 - acc: 0.9932 - val_loss: 0.0967 - val_acc: 0.9765\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 4s 64us/step - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0783 - val_acc: 0.9787\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 3s 63us/step - loss: 0.0129 - acc: 0.9957 - val_loss: 0.0858 - val_acc: 0.9781\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 3s 63us/step - loss: 0.0161 - acc: 0.9945 - val_loss: 0.0978 - val_acc: 0.9788\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 3s 62us/step - loss: 0.0146 - acc: 0.9954 - val_loss: 0.0877 - val_acc: 0.9793\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 4s 64us/step - loss: 0.0131 - acc: 0.9959 - val_loss: 0.0992 - val_acc: 0.9762\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 4s 67us/step - loss: 0.0134 - acc: 0.9958 - val_loss: 0.0693 - val_acc: 0.9831\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 4s 68us/step - loss: 0.0103 - acc: 0.9972 - val_loss: 0.0820 - val_acc: 0.9823\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 4s 68us/step - loss: 0.0077 - acc: 0.9978 - val_loss: 0.0917 - val_acc: 0.9826\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 4s 72us/step - loss: 0.0121 - acc: 0.9964 - val_loss: 0.0980 - val_acc: 0.9796\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 4s 72us/step - loss: 0.0129 - acc: 0.9961 - val_loss: 0.0902 - val_acc: 0.9800\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 4s 68us/step - loss: 0.0109 - acc: 0.9967 - val_loss: 0.0793 - val_acc: 0.9836\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 4s 64us/step - loss: 0.0052 - acc: 0.9986 - val_loss: 0.1053 - val_acc: 0.9803\n",
      "10000/10000 [==============================] - 0s 29us/step\n",
      "\n",
      "acc: 98.03%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras as keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import advanced_activations\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 20\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "#\n",
    "activationFun = 'relu'\n",
    "#activationFun = 'softmax'\n",
    "\n",
    "tryOverfit = False\n",
    "\n",
    "#Definition of the model type\n",
    "model = Sequential()\n",
    "#Definition of the layers\n",
    "model.add(Dense(num_input, input_dim=num_input,activation=activationFun))\n",
    "model.add(Dense(n_hidden_1,activation=activationFun))\n",
    "model.add(Dense(n_hidden_2,activation=activationFun))\n",
    "if tryOverfit:\n",
    "    model.add(Dense(n_hidden_2,activation=activationFun))\n",
    "    model.add(Dense(n_hidden_2,activation=activationFun))\n",
    "    model.add(Dense(n_hidden_2,activation=activationFun))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "#Creation of the model\n",
    "adam = Adam(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam,metrics=['accuracy'])\n",
    "#Fit of the network\n",
    "X = mnist.train.images\n",
    "Y = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "Y_test = mnist.test.labels\n",
    "history = model.fit(X, Y, epochs=num_steps, batch_size=batch_size,validation_data=(X_test,Y_test))#validation_split=0.05\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Warning\n",
    "\n",
    "Keras is an API, so the implementation of function is not one to one with TensorFlow. Often similar parameters can bring to different results.\n",
    "\n",
    "**You have to optimize the network for Keras, not for Tensorflow**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analysis' Results\n",
    "\n",
    "If we take a closer look to the results of the network what we can see?\n",
    "\n",
    "During the fit of the model, with `history = model.fit`, we have stored in `history` all the values of **loss** and **accuracy** for train and test.\n",
    "\n",
    "Note that must pass some validation data or a validation split using `validation_data` or `validation_split` parameters in `model.fit` \n",
    "\n",
    "We can define these and other metrics to be use using the parameter `metrics` in `model.compile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.103425</td>\n",
       "      <td>0.9678</td>\n",
       "      <td>0.232174</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.105860</td>\n",
       "      <td>0.9670</td>\n",
       "      <td>0.084149</td>\n",
       "      <td>0.973764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.075614</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.056409</td>\n",
       "      <td>0.981800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070342</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.039610</td>\n",
       "      <td>0.986982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.090939</td>\n",
       "      <td>0.9741</td>\n",
       "      <td>0.033351</td>\n",
       "      <td>0.989018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.069863</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.024377</td>\n",
       "      <td>0.992109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.079910</td>\n",
       "      <td>0.9786</td>\n",
       "      <td>0.022430</td>\n",
       "      <td>0.992436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.096666</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>0.994309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.074285</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.017891</td>\n",
       "      <td>0.993600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.086267</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.016958</td>\n",
       "      <td>0.994600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.076131</td>\n",
       "      <td>0.9810</td>\n",
       "      <td>0.014899</td>\n",
       "      <td>0.995055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.094982</td>\n",
       "      <td>0.9776</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.994745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.098563</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.012435</td>\n",
       "      <td>0.996018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.082009</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>0.012425</td>\n",
       "      <td>0.996073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.077091</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.012540</td>\n",
       "      <td>0.995945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.084559</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.998582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.092772</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.010184</td>\n",
       "      <td>0.996927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.079462</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.012545</td>\n",
       "      <td>0.996564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.079104</td>\n",
       "      <td>0.9846</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.996818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.100212</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>0.997327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_loss  val_acc      loss       acc\n",
       "0   0.103425   0.9678  0.232174  0.930000\n",
       "1   0.105860   0.9670  0.084149  0.973764\n",
       "2   0.075614   0.9769  0.056409  0.981800\n",
       "3   0.070342   0.9806  0.039610  0.986982\n",
       "4   0.090939   0.9741  0.033351  0.989018\n",
       "5   0.069863   0.9802  0.024377  0.992109\n",
       "6   0.079910   0.9786  0.022430  0.992436\n",
       "7   0.096666   0.9753  0.017708  0.994309\n",
       "8   0.074285   0.9812  0.017891  0.993600\n",
       "9   0.086267   0.9785  0.016958  0.994600\n",
       "10  0.076131   0.9810  0.014899  0.995055\n",
       "11  0.094982   0.9776  0.016428  0.994745\n",
       "12  0.098563   0.9800  0.012435  0.996018\n",
       "13  0.082009   0.9824  0.012425  0.996073\n",
       "14  0.077091   0.9833  0.012540  0.995945\n",
       "15  0.084559   0.9840  0.004474  0.998582\n",
       "16  0.092772   0.9797  0.010184  0.996927\n",
       "17  0.079462   0.9818  0.012545  0.996564\n",
       "18  0.079104   0.9846  0.009900  0.996818\n",
       "19  0.100212   0.9815  0.008335  0.997327"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "history_dict=history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'ro')\n",
    "plt.plot(epochs, val_loss_values, 'b+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_values = history_dict['acc']\n",
    "val_loss_values = history_dict['val_acc']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'ro')\n",
    "plt.plot(epochs, val_loss_values, 'b+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras Single Class\n",
    "\n",
    "Now we'll make an example of a binary classification task using Keras.\n",
    "\n",
    "The dataset is hosted on [Kaggle](https://www.kaggle.com/) and it's the [Promotion Response for a New Product](https://www.kaggle.com/regivm/promotion-response-and-target-datasets/version/1).\n",
    "\n",
    "The description of th dataset is the following:\n",
    "\n",
    "*The context of this business problem is new product introduction. A business organization developed a new product and promoted this to its existing customers. Initially it chose a sample of customers for promotion and the response information is available in the 'promoted' dataset. The organization is interested in building a model to select the best customers for contacting from the pool of customers not contacted ('target' dataset).*\n",
    "\n",
    "and the columns are:\n",
    "\n",
    "  - customer_id\n",
    "  - **res** (what we want to predict)\n",
    "  - card_tenure\n",
    "  - risk_score\n",
    "  - num_promoted\n",
    "  - avg_bal\n",
    "  - geo_group\n",
    "  - res_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python Class \n",
    "\n",
    "To learn something new, we'll make the code of this model using Python classes.\n",
    "\n",
    "A **Class** is a blueprint for an object, that can contain variables and functions (called method).\n",
    "\n",
    "The characteristics of a Python Class are:\n",
    "  \n",
    "  - `__init__` function\n",
    "  - Properties \n",
    "  - Methods \n",
    "  - `self` parameter\n",
    "  \n",
    "\n",
    "Please note similarity of our class with all what we have used so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reading          card_tenure     risk_score   num_promoted  Unnamed: 7\n",
      "count  107792.000000  110000.000000  110000.000000         0.0\n",
      "mean      138.956564     655.571482       0.006782         NaN\n",
      "std        67.433081      81.252328       0.082183         NaN\n",
      "min        12.000000     520.000000       0.000000         NaN\n",
      "25%        91.000000     600.000000       0.000000         NaN\n",
      "50%       135.000000     678.000000       0.000000         NaN\n",
      "75%       179.000000     720.000000       0.000000         NaN\n",
      "max       641.000000     760.000000       2.000000         NaN\n",
      "After reading                resp   card_tenure    risk_score  num_promoted\n",
      "count  25000.000000  24515.000000  25000.000000  25000.000000\n",
      "mean       0.068640    139.491617    655.091680      0.007000\n",
      "std        0.252846     66.998010     81.315116      0.083374\n",
      "min        0.000000      0.000000    520.000000      0.000000\n",
      "25%        0.000000     95.000000    599.000000      0.000000\n",
      "50%        0.000000    135.000000    677.000000      0.000000\n",
      "75%        0.000000    179.000000    719.000000      0.000000\n",
      "max        1.000000    641.000000    760.000000      1.000000\n",
      "SHAPE OF DF: 22400 X 8\n",
      "SHAPE OF DF2ENC: 22400 X 3\n",
      "SHAPE OF DFENC: 22400 X 3276\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "SHAPE OF DFENC: 22400 X 3276\n",
      "SHAPE OF X_train: 22400 X 3276\n",
      "SHAPE OF dfEnc AFTER ALL: 22400 X 3279\n",
      "AFTER PREPROCESSING dfEnc HAS COLUMUNS Index([             0,              1,              2,              3,\n",
      "                    4,              5,              6,              7,\n",
      "                    8,              9,\n",
      "       ...\n",
      "                 3269,           3270,           3271,           3272,\n",
      "                 3273,           3274,           3275,  'card_tenure',\n",
      "         'risk_score', 'num_promoted'],\n",
      "      dtype='object', length=3279) AND TYPES 0               float64\n",
      "1               float64\n",
      "2               float64\n",
      "3               float64\n",
      "4               float64\n",
      "5               float64\n",
      "6               float64\n",
      "7               float64\n",
      "8               float64\n",
      "9               float64\n",
      "10              float64\n",
      "11              float64\n",
      "12              float64\n",
      "13              float64\n",
      "14              float64\n",
      "15              float64\n",
      "16              float64\n",
      "17              float64\n",
      "18              float64\n",
      "19              float64\n",
      "20              float64\n",
      "21              float64\n",
      "22              float64\n",
      "23              float64\n",
      "24              float64\n",
      "25              float64\n",
      "26              float64\n",
      "27              float64\n",
      "28              float64\n",
      "29              float64\n",
      "                 ...   \n",
      "3249            float64\n",
      "3250            float64\n",
      "3251            float64\n",
      "3252            float64\n",
      "3253            float64\n",
      "3254            float64\n",
      "3255            float64\n",
      "3256            float64\n",
      "3257            float64\n",
      "3258            float64\n",
      "3259            float64\n",
      "3260            float64\n",
      "3261            float64\n",
      "3262            float64\n",
      "3263            float64\n",
      "3264            float64\n",
      "3265            float64\n",
      "3266            float64\n",
      "3267            float64\n",
      "3268            float64\n",
      "3269            float64\n",
      "3270            float64\n",
      "3271            float64\n",
      "3272            float64\n",
      "3273            float64\n",
      "3274            float64\n",
      "3275            float64\n",
      "card_tenure     float64\n",
      "risk_score      float64\n",
      "num_promoted    float64\n",
      "Length: 3279, dtype: object\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 3279)              10755120  \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 25)                82000     \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 10,837,391\n",
      "Trainable params: 10,837,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "train set [ 4481  4482  4483 ... 22397 22398 22399]\n",
      "Epoch 1/15\n",
      "17919/17919 [==============================] - 21s 1ms/step - loss: 0.2626 - acc: 0.9314\n",
      "Epoch 2/15\n",
      "17919/17919 [==============================] - 20s 1ms/step - loss: 0.2155 - acc: 0.9315\n",
      "Epoch 3/15\n",
      "17919/17919 [==============================] - 19s 1ms/step - loss: 0.1840 - acc: 0.9315\n",
      "Epoch 4/15\n",
      "17919/17919 [==============================] - 19s 1ms/step - loss: 0.1525 - acc: 0.9421\n",
      "Epoch 5/15\n",
      "17919/17919 [==============================] - 20s 1ms/step - loss: 0.1358 - acc: 0.9516\n",
      "Epoch 6/15\n",
      "17919/17919 [==============================] - 21s 1ms/step - loss: 0.1279 - acc: 0.9530\n",
      "Epoch 7/15\n",
      "17919/17919 [==============================] - 19s 1ms/step - loss: 0.1197 - acc: 0.9544\n",
      "Epoch 8/15\n",
      "17919/17919 [==============================] - 22s 1ms/step - loss: 0.1142 - acc: 0.9566\n",
      "Epoch 9/15\n",
      "17919/17919 [==============================] - 21s 1ms/step - loss: 0.1071 - acc: 0.9569\n",
      "Epoch 10/15\n",
      "17919/17919 [==============================] - 19s 1ms/step - loss: 0.1014 - acc: 0.9583\n",
      "Epoch 11/15\n",
      "17919/17919 [==============================] - 20s 1ms/step - loss: 0.0971 - acc: 0.9602\n",
      "Epoch 12/15\n",
      "17919/17919 [==============================] - 20s 1ms/step - loss: 0.0936 - acc: 0.9603\n",
      "Epoch 13/15\n",
      "17919/17919 [==============================] - 20s 1ms/step - loss: 0.0925 - acc: 0.9612\n",
      "Epoch 14/15\n",
      "17919/17919 [==============================] - 20s 1ms/step - loss: 0.0924 - acc: 0.9612\n",
      "Epoch 15/15\n",
      "17919/17919 [==============================] - 19s 1ms/step - loss: 0.0906 - acc: 0.9609\n",
      "4481/4481 [==============================] - 1s 253us/step\n",
      "train set [    0     1     2 ... 22397 22398 22399]\n",
      "Epoch 1/15\n",
      "17920/17920 [==============================] - 22s 1ms/step - loss: 0.1703 - acc: 0.9477\n",
      "Epoch 2/15\n",
      "17920/17920 [==============================] - 20s 1ms/step - loss: 0.1309 - acc: 0.9526\n",
      "Epoch 3/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.1136 - acc: 0.9555\n",
      "Epoch 4/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.1051 - acc: 0.9571\n",
      "Epoch 5/15\n",
      "17920/17920 [==============================] - 20s 1ms/step - loss: 0.1002 - acc: 0.9583\n",
      "Epoch 6/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0964 - acc: 0.9597\n",
      "Epoch 7/15\n",
      "17920/17920 [==============================] - 20s 1ms/step - loss: 0.0939 - acc: 0.9600\n",
      "Epoch 8/15\n",
      "17920/17920 [==============================] - 20s 1ms/step - loss: 0.0922 - acc: 0.9606\n",
      "Epoch 9/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0917 - acc: 0.9609\n",
      "Epoch 10/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0910 - acc: 0.9609\n",
      "Epoch 11/15\n",
      "17920/17920 [==============================] - 20s 1ms/step - loss: 0.0899 - acc: 0.9601\n",
      "Epoch 12/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0898 - acc: 0.9610\n",
      "Epoch 13/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0893 - acc: 0.9612\n",
      "Epoch 14/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0885 - acc: 0.9607\n",
      "Epoch 15/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0885 - acc: 0.9611\n",
      "4480/4480 [==============================] - 1s 177us/step\n",
      "train set [    0     1     2 ... 22397 22398 22399]\n",
      "Epoch 1/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.1110 - acc: 0.9573\n",
      "Epoch 2/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0957 - acc: 0.9596\n",
      "Epoch 3/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0927 - acc: 0.9605\n",
      "Epoch 4/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0906 - acc: 0.9608\n",
      "Epoch 5/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0900 - acc: 0.9608\n",
      "Epoch 6/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0880 - acc: 0.9618\n",
      "Epoch 7/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0876 - acc: 0.9619\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17920/17920 [==============================] - 20s 1ms/step - loss: 0.0871 - acc: 0.9624\n",
      "Epoch 9/15\n",
      "17920/17920 [==============================] - 22s 1ms/step - loss: 0.0862 - acc: 0.9628\n",
      "Epoch 10/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0856 - acc: 0.9622\n",
      "Epoch 11/15\n",
      "17920/17920 [==============================] - 20s 1ms/step - loss: 0.0845 - acc: 0.9629\n",
      "Epoch 12/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0847 - acc: 0.9627\n",
      "Epoch 13/15\n",
      "17920/17920 [==============================] - 20s 1ms/step - loss: 0.0836 - acc: 0.9629\n",
      "Epoch 14/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0830 - acc: 0.9627\n",
      "Epoch 15/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0833 - acc: 0.9622\n",
      "4480/4480 [==============================] - 1s 199us/step\n",
      "train set [    0     1     2 ... 22397 22398 22399]\n",
      "Epoch 1/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.1089 - acc: 0.9579\n",
      "Epoch 2/15\n",
      "17920/17920 [==============================] - 21s 1ms/step - loss: 0.0976 - acc: 0.9585\n",
      "Epoch 3/15\n",
      "17920/17920 [==============================] - 18s 1ms/step - loss: 0.0953 - acc: 0.9591\n",
      "Epoch 4/15\n",
      "17920/17920 [==============================] - 57s 3ms/step - loss: 0.0931 - acc: 0.9594\n",
      "Epoch 5/15\n",
      "17920/17920 [==============================] - 18s 1ms/step - loss: 0.0918 - acc: 0.9608\n",
      "Epoch 6/15\n",
      "17920/17920 [==============================] - 18s 1ms/step - loss: 0.0903 - acc: 0.9610\n",
      "Epoch 7/15\n",
      "17920/17920 [==============================] - 18s 995us/step - loss: 0.0903 - acc: 0.9611\n",
      "Epoch 8/15\n",
      "17920/17920 [==============================] - 17s 951us/step - loss: 0.0890 - acc: 0.9613\n",
      "Epoch 9/15\n",
      "17920/17920 [==============================] - 18s 1ms/step - loss: 0.0886 - acc: 0.9616\n",
      "Epoch 10/15\n",
      "17920/17920 [==============================] - 20s 1ms/step - loss: 0.0884 - acc: 0.9621\n",
      "Epoch 11/15\n",
      "17920/17920 [==============================] - 20s 1ms/step - loss: 0.0883 - acc: 0.9611\n",
      "Epoch 12/15\n",
      "17920/17920 [==============================] - 19s 1ms/step - loss: 0.0870 - acc: 0.9614\n",
      "Epoch 13/15\n",
      "17920/17920 [==============================] - 19s 1ms/step - loss: 0.0870 - acc: 0.9618\n",
      "Epoch 14/15\n",
      "17920/17920 [==============================] - 19s 1ms/step - loss: 0.0872 - acc: 0.9614\n",
      "Epoch 15/15\n",
      "17920/17920 [==============================] - 20s 1ms/step - loss: 0.0866 - acc: 0.9607\n",
      "4480/4480 [==============================] - 1s 220us/step\n",
      "train set [    0     1     2 ... 18008 18031 18032]\n",
      "Epoch 1/15\n",
      "17921/17921 [==============================] - 20s 1ms/step - loss: 0.1043 - acc: 0.9590\n",
      "Epoch 2/15\n",
      "17921/17921 [==============================] - 19s 1ms/step - loss: 0.0959 - acc: 0.9597\n",
      "Epoch 3/15\n",
      "17921/17921 [==============================] - 19s 1ms/step - loss: 0.0925 - acc: 0.9602\n",
      "Epoch 4/15\n",
      "17921/17921 [==============================] - 20s 1ms/step - loss: 0.0909 - acc: 0.9616\n",
      "Epoch 5/15\n",
      "17921/17921 [==============================] - 20s 1ms/step - loss: 0.0900 - acc: 0.9614\n",
      "Epoch 6/15\n",
      "17921/17921 [==============================] - 20s 1ms/step - loss: 0.0889 - acc: 0.9617\n",
      "Epoch 7/15\n",
      "17921/17921 [==============================] - 20s 1ms/step - loss: 0.0885 - acc: 0.9621\n",
      "Epoch 8/15\n",
      "17921/17921 [==============================] - 20s 1ms/step - loss: 0.0884 - acc: 0.9621\n",
      "Epoch 9/15\n",
      "17921/17921 [==============================] - 20s 1ms/step - loss: 0.0874 - acc: 0.9617\n",
      "Epoch 10/15\n",
      "17921/17921 [==============================] - 20s 1ms/step - loss: 0.0870 - acc: 0.9624\n",
      "Epoch 11/15\n",
      "17921/17921 [==============================] - 20s 1ms/step - loss: 0.0859 - acc: 0.9626\n",
      "Epoch 12/15\n",
      "17921/17921 [==============================] - 20s 1ms/step - loss: 0.0850 - acc: 0.9624\n",
      "Epoch 13/15\n",
      "17921/17921 [==============================] - 20s 1ms/step - loss: 0.0855 - acc: 0.9624\n",
      "Epoch 14/15\n",
      "17921/17921 [==============================] - 19s 1ms/step - loss: 0.0846 - acc: 0.9629\n",
      "Epoch 15/15\n",
      "17921/17921 [==============================] - 20s 1ms/step - loss: 0.0846 - acc: 0.9634\n",
      "4479/4479 [==============================] - 1s 201us/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import advanced_activations\n",
    "from keras.optimizers import Adam\n",
    "from sklearn import preprocessing\n",
    "import logging\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('pippo')\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 15\n",
    "batch_size = 128\n",
    "n_fold = 5\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 25 # 1st layer number of neurons\n",
    "n_hidden_2 = 10 # 2nd layer number of neurons\n",
    "#num_input = 6 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 1 # MNIST total classes (0-9 digits)\n",
    "\n",
    "trainFile = 'promoted.csv'\n",
    "predFile = 'target.csv'\n",
    "\n",
    "#\n",
    "activationFun = 'relu'\n",
    "#activationFun = 'softmax'\n",
    "\n",
    "testOneHot = True\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self,**kargs):\n",
    "        self.path = 'data'\n",
    "        self.trainFile = kargs['tr']\n",
    "        self.colList = ['avg_bal','geo_group', 'res_type']\n",
    "        \n",
    "    def readFiles(self,fileName):\n",
    "        fullPath = os.path.join(self.path,fileName)\n",
    "        logger.info('READING %s',fullPath)\n",
    "        df = pd.read_csv(fullPath,sep=',',dtype={'avg_bal':'category', 'geo_group':'category', 'res_type':'category',})#dtype={'avg_bal':'category', 'geo_group':'category', 'res_type':'category',}\n",
    "        logger.info('LOADED DATASET WITH SHAPE %s AND COLUMUNS %s',str(df.shape),str(df.columns))\n",
    "        print('After reading',df.describe())\n",
    "        return df\n",
    "    \n",
    "    def useOneHot(self,df):\n",
    "        print('SHAPE OF DF: %i X %i'%(df.shape[0],df.shape[1]))\n",
    "        df2Enc = df.loc[:,self.colList]\n",
    "        print('SHAPE OF DF2ENC: %i X %i'%(df2Enc.shape[0],df2Enc.shape[1]))\n",
    "        enc = OneHotEncoder()\n",
    "        dfEnc = enc.fit_transform(df2Enc)\n",
    "        print('SHAPE OF DFENC: %i X %i'%(dfEnc.shape[0],dfEnc.shape[1]))\n",
    "        print(type(dfEnc))\n",
    "        dfEnc = pd.DataFrame(dfEnc.toarray())\n",
    "        print('SHAPE OF DFENC: %i X %i'%(dfEnc.shape[0],dfEnc.shape[1]))\n",
    "        #df.drop(columns=self.colList,inplace=True)\n",
    "        return dfEnc\n",
    "        \n",
    "        \n",
    "    def prepareTrain(self):\n",
    "        dfTrain = self.readFiles(self.trainFile)\n",
    "        logger.info('REMOVING ROWS WITH NA')\n",
    "        logger.info('NROWS BEFORE REMOVING NA %i',dfTrain.shape[0])\n",
    "        dfTrain.dropna(inplace=True)\n",
    "        logger.info('NROWS AFTER REMOVING NA %i',dfTrain.shape[0])\n",
    "        Y_train = dfTrain.loc[:,'resp']\n",
    "        logger.info('CONSIDERING LEVELS FOR CATEGORICAL COLUMNS')\n",
    "        if testOneHot:\n",
    "            dfEnc = self.useOneHot(dfTrain)\n",
    "            print('SHAPE OF X_train: %i X %i'%(dfEnc.shape[0],dfEnc.shape[1]))\n",
    "        else:\n",
    "            for curCol in self.colList:\n",
    "                X_train[curCol] = dfTrain[curCol].cat.codes\n",
    "        logger.info('SCALING OF NUMERIC COLUMNS')\n",
    "        mmscaler = preprocessing.MinMaxScaler()\n",
    "        for curCol in ['card_tenure', 'risk_score', 'num_promoted']:\n",
    "            curFeat = mmscaler.fit_transform(dfTrain[[curCol]])  \n",
    "            dfEnc[curCol] = curFeat.reshape(-1,1)\n",
    "        print('SHAPE OF dfEnc AFTER ALL: %i X %i'%(dfEnc.shape[0],dfEnc.shape[1]))\n",
    "        print('AFTER PREPROCESSING dfEnc HAS COLUMUNS %s AND TYPES %s'%(str(dfEnc.columns),str(dfEnc.dtypes)))\n",
    "        return dfEnc, Y_train\n",
    "\n",
    "class CreateNN:\n",
    "    def __init__(self,**kargs):\n",
    "        self.X_train = kargs['xt']\n",
    "        self.Y_train = kargs['yt']\n",
    "        self.kFold = kargs['kf']\n",
    "        self.i = 1\n",
    "        self.num_input = self.X_train.shape[1]\n",
    "        \n",
    "    def modelDefinition(self):\n",
    "        logger.info('DEFINITION OF THE MODEL')\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(self.num_input, input_dim = self.num_input,activation=activationFun))\n",
    "        self.model.add(Dense(n_hidden_1,activation = activationFun))\n",
    "        self.model.add(Dense(n_hidden_2,activation = activationFun))\n",
    "        self.model.add(Dense(num_classes,activation = 'sigmoid'))\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    def modelCompile(self):\n",
    "        logger.info('COMPILATION OF THE MODEL')\n",
    "        adam = Adam(lr = learning_rate)\n",
    "        self.model.compile(loss = 'binary_crossentropy', optimizer = adam,metrics = ['accuracy'])\n",
    "        \n",
    "    def modelEval(self):\n",
    "        logger.info('EVALUATION OF THE MODEL')\n",
    "        totalScores = list()\n",
    "        logger.info('START OF THE CROSS VALIDATION')\n",
    "        for train,test in self.kFold.split(self.X_train, self.Y_train):\n",
    "            logger.info('WORKING ON FOLD %i',self.i)\n",
    "            print('train set',train)\n",
    "            history = self.model.fit(self.X_train.iloc[train], self.Y_train.iloc[train],\n",
    "                                     epochs=num_steps, \n",
    "                                     batch_size = batch_size) #validation_data=(self.X_train.iloc[test], self.Y_train.iloc[test])\n",
    "            scores = self.model.evaluate(self.X_train.iloc[test], self.Y_train.iloc[test])\n",
    "            totalScores.append(scores[1])\n",
    "            self.i += 1\n",
    "        return history, self.model, totalScores\n",
    "\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    #Inizialization of the class LoadData \n",
    "    logger.info('INIZIALIZATION OF LOADDATA')\n",
    "    ld = LoadData(tr=trainFile)\n",
    "    df2Pred = ld.readFiles(predFile)\n",
    "    X_train, Y_train = ld.prepareTrain()\n",
    "    kfold = StratifiedKFold(n_splits=n_fold)\n",
    "    logger.info('INIZIALIZATION OF CreateNN')\n",
    "    cnn = CreateNN(xt=X_train,yt=Y_train,kf=kfold)\n",
    "    cnn.modelDefinition()\n",
    "    cnn.modelCompile()\n",
    "    history, model, totalScores = cnn.modelEval() \n",
    "    logger.info('EVALUATION COMPLETED')\n",
    "    logger.info(\"FOR THE ACTUAL MODEL THE RESULTS OF %s IS: %.2f%%+/-%.2f%%\" % (model.metrics_names[1], np.mean(totalScores),np.std(totalScores)))\n",
    "    return X_train,Y_train, history,totalScores\n",
    "    \n",
    "X_train,Y_train, history,totalScores = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise Model\n",
    "\n",
    "Try to understand the best parameters for the above model using what we have learned in this lesson (hint: look if the model is overfitting).\n",
    "\n",
    "After the definition of a good model, try to make the prediction over the pred set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Class\n",
    "\n",
    "Try to re-write the first Keras Model using a *Class* approach.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Custom model\n",
    "\n",
    "The models that we have built in the previous examples are made using the **Sequantial model**: `from keras.models import Sequential`.\n",
    "\n",
    "The sequential model is the standard to create a network with a sequence of layers,one after the other.\n",
    "\n",
    "Aside from that model, Keras give us the possibility to create networks with others layout ([more info about models](https://keras.io/models/about-keras-models/)).\n",
    "\n",
    "This is done with the **functional API** ([more about functional API](https://keras.io/models/model/)).\n",
    "\n",
    "The functional API usable with this import `from keras.models import Model`.\n",
    "\n",
    "**NOTE:** with this kinf of model we are working directly with tensors ([wiki tensor](https://en.wikipedia.org/wiki/Tensor)). \n",
    "\n",
    "In this case we want to build a model that \"simulate\" a linear regression using a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 15.7559 - mean_squared_error: 15.7559\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 822us/step - loss: 0.7632 - mean_squared_error: 0.7632\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 722us/step - loss: 0.0677 - mean_squared_error: 0.0677\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0428 - mean_squared_error: 0.0428\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 932us/step - loss: 0.0425 - mean_squared_error: 0.0425\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0416 - mean_squared_error: 0.0416\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 846us/step - loss: 0.0403 - mean_squared_error: 0.0403\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0375 - mean_squared_error: 0.0375\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0176 - mean_squared_error: 0.0176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.604548</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.918015</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58.231480</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77.544945</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96.858406</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred  real\n",
       "0  19.604548    20\n",
       "1  38.918015    40\n",
       "2  58.231480    60\n",
       "3  77.544945    80\n",
       "4  96.858406   100"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "\n",
    "inputs = Input(shape=(1,))\n",
    "preds = Dense(1,activation='linear')(inputs)\n",
    "x = np.array([1,2,3,4,5])\n",
    "y = x*2\n",
    "x_test = x*10\n",
    "y_test = x_test*2\n",
    "\n",
    "model = Model(inputs=inputs,outputs=preds)\n",
    "sgd=keras.optimizers.SGD()\n",
    "model.compile(optimizer=sgd ,loss='mse',metrics=['mse'])\n",
    "model.fit(x,y, batch_size=1, epochs=30, shuffle=False)\n",
    "#model.predict(x_test).shape\n",
    "res = pd.DataFrame({'pred':model.predict(x_test)[:,0],'real':y_test})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "type(inputs)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
